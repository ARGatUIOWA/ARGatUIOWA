<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://argatuiowa.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://argatuiowa.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-13T16:50:10+00:00</updated><id>https://argatuiowa.github.io/feed.xml</id><title type="html">Algorithms Reading Group</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Yongjian Zhong presents “Can Q-learning be improved with advice?”</title><link href="https://argatuiowa.github.io/blog/2023/Yongjian/" rel="alternate" type="text/html" title="Yongjian Zhong presents “Can Q-learning be improved with advice?”"/><published>2023-11-14T08:30:00+00:00</published><updated>2023-11-14T08:30:00+00:00</updated><id>https://argatuiowa.github.io/blog/2023/Yongjian</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2023/Yongjian/"><![CDATA[<p><strong>Abstract</strong>: Despite rapid progress in theoretical reinforcement learning (RL) over the last few years, most of the known guarantees are worst-case in nature, failing to take advantage of structure that may be known a priori about a given RL problem at hand. In this paper we address the question of whether worst-case lower bounds for regret in online learning of Markov decision processes (MDPs) can be circumvented when information about the MDP, in the form of predictions about its optimal Q-value function, is given to the algorithm. We show that when the predictions about the optimal Q-value function satisfy a reasonably weak condition we call distillation, then we can improve regret bounds by replacing the set of state-action pairs with the set of state-action pairs on which the predictions are grossly inaccurate. This improvement holds for both uniform regret bounds and gap-based ones. Further, we are able to achieve this property with an algorithm that achieves sublinear regret when given arbitrary predictions (i.e., even those which are not a distillation). Our work extends a recent line of work on algorithms with predictions, which has typically focused on simple online problems such as caching and scheduling, to the more complex and general problem of reinforcement learning.</p> <p><a href="https://arxiv.org/pdf/2110.13052.pdf">Can Q-learning be improved with advice?</a></p>]]></content><author><name></name></author><category term="Q-learning"/><summary type="html"><![CDATA[Abstract: Despite rapid progress in theoretical reinforcement learning (RL) over the last few years, most of the known guarantees are worst-case in nature, failing to take advantage of structure that may be known a priori about a given RL problem at hand. In this paper we address the question of whether worst-case lower bounds for regret in online learning of Markov decision processes (MDPs) can be circumvented when information about the MDP, in the form of predictions about its optimal Q-value function, is given to the algorithm. We show that when the predictions about the optimal Q-value function satisfy a reasonably weak condition we call distillation, then we can improve regret bounds by replacing the set of state-action pairs with the set of state-action pairs on which the predictions are grossly inaccurate. This improvement holds for both uniform regret bounds and gap-based ones. Further, we are able to achieve this property with an algorithm that achieves sublinear regret when given arbitrary predictions (i.e., even those which are not a distillation). Our work extends a recent line of work on algorithms with predictions, which has typically focused on simple online problems such as caching and scheduling, to the more complex and general problem of reinforcement learning.]]></summary></entry><entry><title type="html">Sriram Pemmaraju presents “The primal-dual method for learning augmented algorithms”</title><link href="https://argatuiowa.github.io/blog/2023/Sriram/" rel="alternate" type="text/html" title="Sriram Pemmaraju presents “The primal-dual method for learning augmented algorithms”"/><published>2023-10-10T08:30:00+00:00</published><updated>2023-10-10T08:30:00+00:00</updated><id>https://argatuiowa.github.io/blog/2023/Sriram</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2023/Sriram/"><![CDATA[<p><strong>Abstract</strong>: The extension of classical online algorithms when provided with predictions is a new and active research area. In this paper, we extend the primal-dual method for online algorithms in order to incorporate predictions that advise the online algorithm about the next action to take. We use this framework to obtain novel algorithms for a variety of online covering problems. We compare our algorithms to the cost of the true and predicted offline optimal solutions and show that these algorithms outperform any online algorithm when the prediction is accurate while maintaining good guarantees when the prediction is misleading.</p> <p><a href="https://arxiv.org/pdf/2010.11632.pdf">The primal-dual method for learning augmented algorithms</a></p>]]></content><author><name></name></author><category term="primal-dual"/><summary type="html"><![CDATA[Abstract: The extension of classical online algorithms when provided with predictions is a new and active research area. In this paper, we extend the primal-dual method for online algorithms in order to incorporate predictions that advise the online algorithm about the next action to take. We use this framework to obtain novel algorithms for a variety of online covering problems. We compare our algorithms to the cost of the true and predicted offline optimal solutions and show that these algorithms outperform any online algorithm when the prediction is accurate while maintaining good guarantees when the prediction is misleading.]]></summary></entry><entry><title type="html">Hongyan Ji presents “Learning-Augmented Algorithms for Online Steiner Tree”</title><link href="https://argatuiowa.github.io/blog/2023/Hongyan/" rel="alternate" type="text/html" title="Hongyan Ji presents “Learning-Augmented Algorithms for Online Steiner Tree”"/><published>2023-09-26T08:30:00+00:00</published><updated>2023-09-26T08:30:00+00:00</updated><id>https://argatuiowa.github.io/blog/2023/Hongyan</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2023/Hongyan/"><![CDATA[<p><strong>Abstract</strong>: This paper considers the recently popular beyond-worst-case algorithm analysis model which integrates machine-learned predictions with online algorithm design. We consider the on- line Steiner tree problem in this model for both directed and undirected graphs. Steiner tree is known to have strong lower bounds in the online setting and any algorithm’s worst-case guarantee is far from desirable. This paper considers algorithms that predict which terminal arrives online. The predictions may be incorrect and the al- gorithms’ performance is parameterized by the number of in- correctly predicted terminals. These guarantees ensure that algorithms break through the online lower bounds with good predictions and the competitive ratio gracefully degrades as the prediction error grows. We then observe that the theory is predictive of what will occur empirically. We show on graphs where terminals are drawn from a distribution, the new on- line algorithms have strong performance even with modestly correct predictions.</p> <p><a href="https://arxiv.org/pdf/2112.05353.pdf">Learning-Augmented Algorithms for Online Steiner Tree</a></p>]]></content><author><name></name></author><category term="online"/><category term="steiner-tree"/><summary type="html"><![CDATA[Abstract: This paper considers the recently popular beyond-worst-case algorithm analysis model which integrates machine-learned predictions with online algorithm design. We consider the on- line Steiner tree problem in this model for both directed and undirected graphs. Steiner tree is known to have strong lower bounds in the online setting and any algorithm’s worst-case guarantee is far from desirable. This paper considers algorithms that predict which terminal arrives online. The predictions may be incorrect and the al- gorithms’ performance is parameterized by the number of in- correctly predicted terminals. These guarantees ensure that algorithms break through the online lower bounds with good predictions and the competitive ratio gracefully degrades as the prediction error grows. We then observe that the theory is predictive of what will occur empirically. We show on graphs where terminals are drawn from a distribution, the new on- line algorithms have strong performance even with modestly correct predictions.]]></summary></entry><entry><title type="html">Xiang Liu presents “Machine Learning Advised Ski Rental Problem with a Discount”</title><link href="https://argatuiowa.github.io/blog/2023/Xiang/" rel="alternate" type="text/html" title="Xiang Liu presents “Machine Learning Advised Ski Rental Problem with a Discount”"/><published>2023-09-19T08:30:00+00:00</published><updated>2023-09-19T08:30:00+00:00</updated><id>https://argatuiowa.github.io/blog/2023/Xiang</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2023/Xiang/"><![CDATA[<p><strong>Abstract</strong>: Traditional online algorithms are designed to make decisions online in the face of uncertainty to perform well in comparison with the optimal offline algorithm for the worst-case inputs. On the other hand, machine learning algorithms try to extrapolate the pattern from the past inputs to predict the future and take decisions online on basis of the predictions to perform well for the average-case inputs. There have been recent studies to augment traditional online algorithms with machine learning oracles to get better performance for all the possible inputs. The machine learning augmented online algorithms perform provably better than the traditional online algorithms when the error of the machine learning oracle is low for the worst-case inputs and all other average-case inputs. In this paper, we integrate the advantages of the traditional online algorithms and the machine learning algorithms in the context of a novel variant of the ski rental problem. Firstly, we propose the ski rental problem with a discount: in this problem, the rent of the ski, instead of being fixed over time, varies as a function of time. Secondly, we discuss the design and performance evaluation of the online algorithms with machine learning advice to solve the ski rental problem with a discount. Finally, we extend this study to the situation where multiple independent machine learning advice is available. This algorithm design framework motivates to redesign of several online algorithms by augmenting them with one or more machine learning oracles to improve the performance.</p> <p><a href="https://dl.acm.org/doi/abs/10.1007/978-3-030-96731-4_18">Machine Learning Advised Ski Rental Problem with a Discount</a></p>]]></content><author><name></name></author><category term="online"/><category term="ski-rental"/><summary type="html"><![CDATA[Abstract: Traditional online algorithms are designed to make decisions online in the face of uncertainty to perform well in comparison with the optimal offline algorithm for the worst-case inputs. On the other hand, machine learning algorithms try to extrapolate the pattern from the past inputs to predict the future and take decisions online on basis of the predictions to perform well for the average-case inputs. There have been recent studies to augment traditional online algorithms with machine learning oracles to get better performance for all the possible inputs. The machine learning augmented online algorithms perform provably better than the traditional online algorithms when the error of the machine learning oracle is low for the worst-case inputs and all other average-case inputs. In this paper, we integrate the advantages of the traditional online algorithms and the machine learning algorithms in the context of a novel variant of the ski rental problem. Firstly, we propose the ski rental problem with a discount: in this problem, the rent of the ski, instead of being fixed over time, varies as a function of time. Secondly, we discuss the design and performance evaluation of the online algorithms with machine learning advice to solve the ski rental problem with a discount. Finally, we extend this study to the situation where multiple independent machine learning advice is available. This algorithm design framework motivates to redesign of several online algorithms by augmenting them with one or more machine learning oracles to improve the performance.]]></summary></entry><entry><title type="html">Haakon Larsen presents “Improving Online Algorithms via ML Predictions”</title><link href="https://argatuiowa.github.io/blog/2023/Haakon/" rel="alternate" type="text/html" title="Haakon Larsen presents “Improving Online Algorithms via ML Predictions”"/><published>2023-09-12T08:30:00+00:00</published><updated>2023-09-12T08:30:00+00:00</updated><id>https://argatuiowa.github.io/blog/2023/Haakon</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2023/Haakon/"><![CDATA[<p><strong>Abstract</strong>: In this work we study the problem of using machine-learned predictions to improve the performance of online algorithms. We consider two classical problems, ski rental and non-clairvoyant job scheduling, and obtain new online algorithms that use predictions to make their decisions. These algorithms are oblivious to the performance of the predictor, improve with better predictions, but do not degrade much if the predictions are poor.</p> <p><a href="https://dl.acm.org/doi/pdf/10.5555/3327546.3327635">Improving Online Algorithms via ML Predictions</a></p>]]></content><author><name></name></author><category term="online"/><category term="ski-rental"/><category term="non-clairvoyant-job-scheduling"/><summary type="html"><![CDATA[Abstract: In this work we study the problem of using machine-learned predictions to improve the performance of online algorithms. We consider two classical problems, ski rental and non-clairvoyant job scheduling, and obtain new online algorithms that use predictions to make their decisions. These algorithms are oblivious to the performance of the predictor, improve with better predictions, but do not degrade much if the predictions are poor.]]></summary></entry><entry><title type="html">Bijaya Adhikari presents “Algorithms with predictions”</title><link href="https://argatuiowa.github.io/blog/2023/post-bibliography/" rel="alternate" type="text/html" title="Bijaya Adhikari presents “Algorithms with predictions”"/><published>2023-09-05T08:30:00+00:00</published><updated>2023-09-05T08:30:00+00:00</updated><id>https://argatuiowa.github.io/blog/2023/post-bibliography</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2023/post-bibliography/"><![CDATA[<p><strong>Abstract</strong>: We introduce algorithms that use predictions from machine learning applied to the input to circumvent worst-case analysis. We aim for algorithms that have near optimal performance when these predictions are good, but recover the prediction-less worst case behavior when the predictions have large errors.</p> <p><a href="http://www.cs.toronto.edu/~bor/2421s21/papers/mitzenmacher-survey.pdf">Algorithms with Predictions</a></p>]]></content><author><name></name></author><category term="online"/><category term="survey"/><summary type="html"><![CDATA[Abstract: We introduce algorithms that use predictions from machine learning applied to the input to circumvent worst-case analysis. We aim for algorithms that have near optimal performance when these predictions are good, but recover the prediction-less worst case behavior when the predictions have large errors.]]></summary></entry></feed>